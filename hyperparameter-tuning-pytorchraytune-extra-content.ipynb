{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%writefile config.py\nimport os\n\n# For scheduler\nMAX_NUM_EPOCHS = 50\nGRACE_PERIOD = 1\n\n# Training parameters.\nEPOCHS = 50\n# Data root.\nDATA_ROOT_DIR = os.path.abspath('../input/blood-cells/dataset2-master/dataset2-master/images')\n# Number of parallel processes for data fetching.\nNUM_WORKERS = 2\n\n# For search run.\nCPU = 1\nGPU = 1\nNUM_SAMPLES = 5","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-04T00:27:57.617366Z","iopub.execute_input":"2021-12-04T00:27:57.617708Z","iopub.status.idle":"2021-12-04T00:27:57.654379Z","shell.execute_reply.started":"2021-12-04T00:27:57.617619Z","shell.execute_reply":"2021-12-04T00:27:57.65316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile datasets.py\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\n\nfrom torch.utils.data import DataLoader\n\n# Training transforms.\ndef train_transforms():\n    train_transform = transforms.Compose([\n        transforms.Resize(224),\n        # transforms.RandomHorizontalFlip(p=0.5),\n        # transforms.RandomVerticalFlip(p=0.5),\n        # transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)),\n        # transforms.RandomRotation(degrees=(30, 70)),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5]\n        )\n    ])\n    return train_transform\n\n# Validation transforms.\ndef valid_transforms():\n    valid_transform = transforms.Compose([\n        transforms.Resize(224),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            mean=[0.5, 0.5, 0.5],\n            std=[0.5, 0.5, 0.5]\n        )\n    ])\n    return valid_transform\n\ndef get_datasets(DATA_ROOT_DIR):\n    # Training dataset.\n    train_dataset = datasets.ImageFolder(\n        root=f\"{DATA_ROOT_DIR}/TRAIN\",\n        transform=train_transforms()\n    )\n    # Validation dataset.\n    valid_dataset = datasets.ImageFolder(\n        root=f\"{DATA_ROOT_DIR}/TEST_SIMPLE\",\n        transform=valid_transforms()\n    )\n    # Test dataset.\n    test_dataset = datasets.ImageFolder(\n        root=f\"{DATA_ROOT_DIR}/TEST\",\n        transform=valid_transforms()\n    )\n    return (\n        train_dataset, valid_dataset, \n        test_dataset, train_dataset.classes\n    )\n\ndef get_data_loaders(\n    train_dataset, valid_dataset, test_dataset,\n    BATCH_SIZE, NUM_WORKERS\n):\n    # Training data loader.\n    train_loader = DataLoader(\n        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n        num_workers=NUM_WORKERS, pin_memory=True\n    )\n    # Validation data loader.\n    valid_loader = DataLoader(\n        valid_dataset, batch_size=BATCH_SIZE, shuffle=False,\n        num_workers=NUM_WORKERS, pin_memory=True\n    )\n    # Test data loader.\n    test_loader = DataLoader(\n        test_dataset, batch_size=BATCH_SIZE, shuffle=False,\n        num_workers=NUM_WORKERS, pin_memory=True\n    )\n    return train_loader, valid_loader, test_loader","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:27:57.656856Z","iopub.execute_input":"2021-12-04T00:27:57.657213Z","iopub.status.idle":"2021-12-04T00:27:57.665914Z","shell.execute_reply.started":"2021-12-04T00:27:57.65717Z","shell.execute_reply":"2021-12-04T00:27:57.664739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile model.py\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch\n\nclass CustomNet(nn.Module):\n    def __init__(self, first_conv_out=16, first_fc_out=128):\n        super().__init__()\n\n        self.first_conv_out = first_conv_out\n        self.first_fc_out = first_fc_out\n\n        # All Conv layers.\n        self.conv1 = nn.Conv2d(3, self.first_conv_out, 5)\n        self.conv2 = nn.Conv2d(self.first_conv_out, self.first_conv_out*2, 3)\n        self.conv3 = nn.Conv2d(self.first_conv_out*2, self.first_conv_out*4, 3)\n        self.conv4 = nn.Conv2d(self.first_conv_out*4, self.first_conv_out*8, 3)\n        self.conv5 = nn.Conv2d(self.first_conv_out*8, self.first_conv_out*16, 3)\n\n        # All fully connected layers.\n        self.fc1 = nn.Linear(self.first_conv_out*16, self.first_fc_out)\n        self.fc2 = nn.Linear(self.first_fc_out, self.first_fc_out//2)\n        self.fc3 = nn.Linear(self.first_fc_out//2, 4)\n\n        # Max pooling layers\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x):    \n        # Passing though convolutions.\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.pool(F.relu(self.conv3(x)))\n        x = self.pool(F.relu(self.conv4(x)))\n        x = self.pool(F.relu(self.conv5(x)))\n\n        # Flatten.\n        bs, _, _, _ = x.shape\n        x = F.adaptive_avg_pool2d(x, 1).reshape(bs, -1)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nif __name__ == '__main__':\n    model = CustomNet(32, 512)\n    tensor = torch.randn(1, 3, 224, 224)\n    output = model(tensor)\n    print(output.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:27:57.667829Z","iopub.execute_input":"2021-12-04T00:27:57.668421Z","iopub.status.idle":"2021-12-04T00:27:57.681311Z","shell.execute_reply.started":"2021-12-04T00:27:57.668348Z","shell.execute_reply":"2021-12-04T00:27:57.680186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile train_utils.py\nimport torch\n\nfrom tqdm import tqdm\n\n# Training function.\ndef train(model, data_loader, optimizer, criterion, device):\n    model.train()\n    print('Training')\n    train_running_loss = 0.0\n    train_running_correct = 0\n    counter = 0\n    for i, data in enumerate(data_loader):\n        counter += 1\n        image, labels = data\n        image = image.to(device)\n        labels = labels.to(device)\n        optimizer.zero_grad()\n        # Forward pass.\n        outputs = model(image)\n        # Calculate the loss.\n        loss = criterion(outputs, labels)\n        train_running_loss += loss.item()\n        # Calculate the accuracy.\n        _, preds = torch.max(outputs.data, 1)\n        train_running_correct += (preds == labels).sum().item()\n        # Backpropagation.\n        loss.backward()\n        # Update the optimizer parameters.\n        optimizer.step()\n    \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = train_running_loss / counter\n    epoch_acc = 100. * (train_running_correct / len(data_loader.dataset))\n    return epoch_loss, epoch_acc\n\n# Validation function.\ndef validate(model, data_loader, criterion, device):\n    model.eval()\n    print('Validation')\n    valid_running_loss = 0.0\n    valid_running_correct = 0\n    counter = 0\n    \n    with torch.no_grad():\n        for i, data in enumerate(data_loader):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # Forward pass.\n            outputs = model(image)\n            # Calculate the loss.\n            loss = criterion(outputs, labels)\n            valid_running_loss += loss.item()\n            # Calculate the accuracy.\n            _, preds = torch.max(outputs.data, 1)\n            valid_running_correct += (preds == labels).sum().item()\n        \n    # Loss and accuracy for the complete epoch.\n    epoch_loss = valid_running_loss / counter\n    epoch_acc = 100. * (valid_running_correct / len(data_loader.dataset))\n    return epoch_loss, epoch_acc\n\n# Function to carry out testing.\ndef test(model, data_loader, device):\n    counter = 0\n    test_running_correct = 0.\n    with torch.no_grad():\n        for i, data in tqdm(enumerate(data_loader), total=len(data_loader)):\n            counter += 1\n            \n            image, labels = data\n            image = image.to(device)\n            labels = labels.to(device)\n            # Forward pass.\n            outputs = model(image)\n            # Calculate the accuracy.\n            _, preds = torch.max(outputs.data, 1)\n            test_running_correct += (preds == labels).sum().item()\n        \n    # Loss and accuracy for the complete test set.\n    acc = 100. * (test_running_correct / len(data_loader.dataset))\n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:27:57.683961Z","iopub.execute_input":"2021-12-04T00:27:57.684826Z","iopub.status.idle":"2021-12-04T00:27:57.697136Z","shell.execute_reply.started":"2021-12-04T00:27:57.684779Z","shell.execute_reply":"2021-12-04T00:27:57.695825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile search_and_train.py\nfrom train_utils import train, validate, test\nfrom model import CustomNet\nfrom datasets import get_datasets, get_data_loaders\nfrom config import (\n    MAX_NUM_EPOCHS, GRACE_PERIOD, EPOCHS, CPU, GPU,\n    NUM_SAMPLES, DATA_ROOT_DIR, NUM_WORKERS\n)\n\nfrom ray import tune\nfrom ray.tune import CLIReporter\nfrom ray.tune.schedulers import ASHAScheduler\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport torch.optim as optim\nimport os\n\n\ndef train_and_validate(config):\n    # Get all the datasets\n    (\n        train_dataset, valid_dataset, test_dataset, class_names\n    ) = get_datasets(DATA_ROOT_DIR)\n    print(f\"[INFO]: Number of training samples: {len(train_dataset)}\")\n    print(f\"[INFO]: Number of validation samples: {len(valid_dataset)}\")\n    print(f\"[INFO]: Number of test samples: {len(test_dataset)}\")\n    # Get training and validation data loaders,\n    # ignore test data loader for now.\n    train_loader, valid_loader, _ = get_data_loaders(\n        train_dataset, valid_dataset, test_dataset,\n        config['batch_size'], NUM_WORKERS\n    )\n\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    # Initialize the model\n    model = CustomNet(\n        config['first_conv_out'], config['first_fc_out']\n    ).to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(\n        model.parameters(), lr=config['lr'], momentum=0.9\n    )\n\n    # start the training\n    for epoch in range(EPOCHS):\n        print(f\"[INFO]: Epoch {epoch+1} of {EPOCHS}\")\n        train_epoch_loss, train_epoch_acc = train(\n            model, train_loader, optimizer, criterion, device\n        )\n        valid_epoch_loss, valid_epoch_acc = validate(\n            model, valid_loader, criterion, device\n        )\n  \n        print(f\"Training loss: {train_epoch_loss:.3f}, training acc: {train_epoch_acc:.3f}\")\n        print(f\"Validation loss: {valid_epoch_loss:.3f}, validation acc: {valid_epoch_acc:.3f}\")\n        print('-'*50)\n\n        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n            path = os.path.join(checkpoint_dir, 'checkpoint')\n            torch.save((model.state_dict(), optimizer.state_dict()), path)\n        tune.report(\n            loss=valid_epoch_loss, accuracy=valid_epoch_acc\n        )\n\ndef run_search():\n    # Define the parameter search configuration.\n    config = {\n        \"first_conv_out\": \n            tune.sample_from(lambda _: 2 ** np.random.randint(4, 8)),\n        \"first_fc_out\": \n            tune.sample_from(lambda _: 2 ** np.random.randint(4, 8)),\n        \"lr\": tune.loguniform(1e-4, 1e-1),\n        \"batch_size\": tune.choice([2, 4, 8, 16])\n    }\n\n    # Schduler to stop bad performing trails.\n    scheduler = ASHAScheduler(\n        metric=\"loss\",\n        mode=\"min\",\n        max_t=MAX_NUM_EPOCHS,\n        grace_period=GRACE_PERIOD,\n        reduction_factor=2)\n\n    # Reporter to show on command line/output window\n    reporter = CLIReporter(\n        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n    # Start run/search\n    result = tune.run(\n        train_and_validate,\n        resources_per_trial={\"cpu\": CPU, \"gpu\": GPU},\n        config=config,\n        num_samples=NUM_SAMPLES,\n        scheduler=scheduler,\n        local_dir='raytune_result',\n        keep_checkpoints_num=1,\n        checkpoint_score_attr='min-validation_loss',\n        progress_reporter=reporter\n    )\n\n    # Extract the best trial run from the search.\n    best_trial = result.get_best_trial(\n        'loss', 'min', 'last'\n    )\n    print(f\"Best trial config: {best_trial.config}\")\n    print(f\"Best trial final validation loss: {best_trial.last_result['loss']}\")\n    print(f\"Best trial final validation acc: {best_trial.last_result['accuracy']}\")\n\n    # Carry out the final testing with the best settings.\n    device = 'cuda:0'\n    train_dataset, valid_dataset, test_dataset, _ = get_datasets(\n        DATA_ROOT_DIR\n    )\n    _, _, test_loader = get_data_loaders(\n        train_dataset, valid_dataset, test_dataset, \n        best_trial.config['batch_size'], NUM_WORKERS\n    )\n    print('[INFO]: Building best model for testing...')\n    best_model = CustomNet(\n        best_trial.config['first_conv_out'], \n        best_trial.config['first_fc_out']\n    ).to(device)\n    best_checkpoint_dir = best_trial.checkpoint.value\n    print('[INFO]: Loading best model weights...')\n    model_state, optimizer_state = torch.load(\n        os.path.join(best_checkpoint_dir, 'checkpoint')\n    )\n    best_model.load_state_dict(model_state)\n    test_acc = test(best_model, test_loader, device)\n    print(f\"[INFO]: Test results from the best trained model: {test_acc:.3f}\")\n\nif __name__ == '__main__':\n    run_search()","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:27:57.866281Z","iopub.execute_input":"2021-12-04T00:27:57.866564Z","iopub.status.idle":"2021-12-04T00:27:57.876194Z","shell.execute_reply.started":"2021-12-04T00:27:57.866532Z","shell.execute_reply":"2021-12-04T00:27:57.874867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python search_and_train.py","metadata":{"execution":{"iopub.status.busy":"2021-12-04T00:27:57.878849Z","iopub.execute_input":"2021-12-04T00:27:57.879717Z"},"trusted":true},"execution_count":null,"outputs":[]}]}